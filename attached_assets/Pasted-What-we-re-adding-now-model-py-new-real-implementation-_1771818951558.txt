What we’re adding now
model.py (new real implementation)

It will:

Run a walk-forward backtest (time-based, no leakage)

Compare:

Planet-feature forecast model

Baseline model (historical frequency only)

Score outcomes on historical draws

Produce a plain-English summary

Backtest design (V1)

For each test draw:

Train on all prior draws only ✅

Build feature rules from training set only ✅

Generate forecasted top game cards for that draw date ✅

Compare predicted cards to actual draw:

main-number matches (0–7)

Powerball match (0/1)

total match score

Compare to a baseline set of cards generated from frequency only

This gives you a realistic “would this have helped?” framework.

1) Create / replace model.py
# model.py
import json
from datetime import datetime
from dateutil import tz
import pandas as pd
import numpy as np

from analysis import feature_number_scan, feature_powerball_scan
from forecast import (
    score_numbers_from_rules,
    generate_top_game_cards_for_draw,
)
from ephemeris import EphemerisEngine
from features import build_features

SYDNEY_TZ = tz.gettz("Australia/Sydney")


def _parse_iso_dt(s: str):
    return datetime.fromisoformat(str(s))


def _main_count(numbers_json_str):
    try:
        return len(json.loads(numbers_json_str))
    except Exception:
        return 0


def filter_modern_draws(draws_df: pd.DataFrame):
    """Current format proxy: exactly 7 main numbers."""
    d = draws_df.copy()
    d["main_count"] = d["numbers_json"].apply(_main_count)
    d = d[d["main_count"] == 7].copy()
    d = d.sort_values("draw_datetime_utc").reset_index(drop=True)
    return d


def build_feature_rows_for_draw_subset(draws_subset: pd.DataFrame, feats_df: pd.DataFrame):
    """Keep only feature rows that match the draw subset."""
    ids = set(draws_subset["draw_id"].astype(int).tolist())
    f = feats_df[feats_df["draw_id"].astype(int).isin(ids)].copy()
    return f


def _historical_main_baseline_probs(train_draws: pd.DataFrame, number_min=1, number_max=35):
    counts = {n: 0 for n in range(number_min, number_max + 1)}
    total = 0
    for _, row in train_draws.iterrows():
        nums = json.loads(row["numbers_json"])
        if len(nums) != 7:
            continue
        total += 1
        for n in nums:
            if number_min <= n <= number_max:
                counts[n] += 1
    if total == 0:
        return {n: 1.0 / (number_max - number_min + 1) for n in counts}
    return {n: counts[n] / total for n in counts}


def _historical_pb_baseline_probs(train_draws: pd.DataFrame, pb_min=1, pb_max=20):
    counts = {n: 0 for n in range(pb_min, pb_max + 1)}
    total = 0
    for _, row in train_draws.iterrows():
        pb = int(row["powerball"])
        if pb_min <= pb <= pb_max:
            counts[pb] += 1
            total += 1
    if total == 0:
        return {n: 1.0 / (pb_max - pb_min + 1) for n in counts}
    return {n: counts[n] / total for n in counts}


def _baseline_scored_main(train_draws: pd.DataFrame):
    base = _historical_main_baseline_probs(train_draws)
    rows = []
    for n, p in base.items():
        rows.append({"number": int(n), "score": float(p), "prob_like": float(p)})
    return pd.DataFrame(rows).sort_values(["score", "number"], ascending=[False, True]).reset_index(drop=True)


def _baseline_scored_pb(train_draws: pd.DataFrame):
    base = _historical_pb_baseline_probs(train_draws)
    rows = []
    for n, p in base.items():
        rows.append({"powerball": int(n), "score": float(p), "prob_like": float(p)})
    return pd.DataFrame(rows).sort_values(["score", "powerball"], ascending=[False, True]).reset_index(drop=True)


def _score_prediction_vs_actual(pred_main, pred_pb, actual_main, actual_pb):
    pred_set = set(int(x) for x in pred_main)
    actual_set = set(int(x) for x in actual_main)

    main_matches = len(pred_set.intersection(actual_set))
    pb_match = int(int(pred_pb) == int(actual_pb))

    # Simple weighted score (tune later if you want)
    total_score = main_matches + (1.0 * pb_match)

    return {
        "main_matches": int(main_matches),
        "powerball_match": int(pb_match),
        "total_match_score": float(total_score),
    }


def _prepare_future_features_for_known_draw(test_row: pd.Series, lat: float, lon: float, alt_m: float):
    """
    For backtesting, compute features at the exact historical draw timestamp (same path as future forecasts).
    """
    engine = EphemerisEngine()
    dt_utc = _parse_iso_dt(test_row["draw_datetime_utc"])
    positions = engine.compute_positions(dt_utc, lat, lon, alt_m)
    feats = build_features(positions, bin_size=30, orb_deg=3.0)
    return feats


def _build_planet_scored_tables(train_draws, train_feats, active_features, q_max=0.10, min_lift=0.0, alpha=2.0):
    # Build rules from TRAIN only
    main_rules = feature_number_scan(train_draws, train_feats, number_min=1, number_max=35)
    pb_rules = feature_powerball_scan(train_draws, train_feats, pb_min=1, pb_max=20)

    if len(main_rules):
        main_rules = main_rules[(main_rules["q_value_bh"] <= q_max) & (main_rules["lift"] >= min_lift)].copy()
    if len(pb_rules):
        pb_rules = pb_rules[(pb_rules["q_value_bh"] <= q_max) & (pb_rules["lift"] >= min_lift)].copy()

    # Baselines from TRAIN only
    main_base = _historical_main_baseline_probs(train_draws)
    pb_base = _historical_pb_baseline_probs(train_draws)

    scored_main = score_numbers_from_rules(
        active_features=active_features,
        rules_df=main_rules,
        baseline_probs=main_base,
        q_max=q_max,
        min_lift=min_lift,
        alpha=alpha,
        target_col="number",
    )

    scored_pb = score_numbers_from_rules(
        active_features=active_features,
        rules_df=pb_rules,
        baseline_probs=pb_base,
        q_max=q_max,
        min_lift=min_lift,
        alpha=alpha,
        target_col="powerball",
    )

    return scored_main, scored_pb, main_rules, pb_rules


def _build_baseline_only_scored_tables(train_draws):
    return _baseline_scored_main(train_draws), _baseline_scored_pb(train_draws)


def walk_forward_backtest(
    draws_df: pd.DataFrame,
    feats_df: pd.DataFrame,
    lat: float,
    lon: float,
    alt_m: float,
    test_draws_count: int = 20,
    min_train_draws: int = 100,
    q_max: float = 0.10,
    min_lift: float = 0.0,
    alpha: float = 2.0,
    top_n_games_per_draw: int = 10,
    combo_pool_main_n: int = 14,
    pb_candidates_n: int = 3,
):
    """
    Walk-forward backtest:
      - For each test draw, train on earlier draws only
      - Compare planet-model vs baseline on exact historical outcome
    Returns:
      detail_df, summary_df
    """
    draws = filter_modern_draws(draws_df)
    feats = build_feature_rows_for_draw_subset(draws, feats_df)

    if len(draws) < (min_train_draws + 1):
        return pd.DataFrame(), pd.DataFrame([{
            "error": f"Not enough modern-format draws. Need > {min_train_draws+1}, found {len(draws)}"
        }])

    # Use last N draws as test window
    test_start_idx = max(min_train_draws, len(draws) - test_draws_count)
    detail_rows = []

    for i in range(test_start_idx, len(draws)):
        test_row = draws.iloc[i]
        train_draws = draws.iloc[:i].copy()
        train_feats = build_feature_rows_for_draw_subset(train_draws, feats)

        if len(train_draws) < min_train_draws or len(train_feats) == 0:
            continue

        # Compute active features at exact historical draw timestamp (same forecast path)
        active_features = _prepare_future_features_for_known_draw(test_row, lat, lon, alt_m)

        # Planet model scored tables and rules from TRAIN only
        planet_scored_main, planet_scored_pb, main_rules_used, pb_rules_used = _build_planet_scored_tables(
            train_draws, train_feats, active_features, q_max=q_max, min_lift=min_lift, alpha=alpha
        )

        # Baseline scored tables from TRAIN only
        base_scored_main, base_scored_pb = _build_baseline_only_scored_tables(train_draws)

        # Generate top cards for each model
        planet_cards = generate_top_game_cards_for_draw(
            scored_main_df=planet_scored_main,
            scored_pb_df=planet_scored_pb,
            top_n_games=top_n_games_per_draw,
            combo_pool_main_n=combo_pool_main_n,
            pb_candidates_n=pb_candidates_n,
        )

        baseline_cards = generate_top_game_cards_for_draw(
            scored_main_df=base_scored_main.rename(columns={"number": "number"}),
            scored_pb_df=base_scored_pb.rename(columns={"powerball": "powerball"}),
            top_n_games=top_n_games_per_draw,
            combo_pool_main_n=combo_pool_main_n,
            pb_candidates_n=pb_candidates_n,
        )

        actual_main = json.loads(test_row["numbers_json"])
        actual_pb = int(test_row["powerball"])

        # Score all cards and record best / avg for each model
        def eval_cards(cards_df, model_name):
            if cards_df is None or len(cards_df) == 0:
                return {
                    "model": model_name,
                    "best_main_matches": np.nan,
                    "best_powerball_match": np.nan,
                    "best_total_match_score": np.nan,
                    "avg_main_matches": np.nan,
                    "avg_powerball_match": np.nan,
                    "avg_total_match_score": np.nan,
                    "top_card_main_numbers": None,
                    "top_card_powerball": None,
                    "top_card_confidence": np.nan,
                    "rule_count_main": int(len(main_rules_used)) if model_name == "planet" else 0,
                    "rule_count_powerball": int(len(pb_rules_used)) if model_name == "planet" else 0,
                }

            scored = []
            for _, r in cards_df.iterrows():
                s = _score_prediction_vs_actual(r["main_numbers"], int(r["powerball"]), actual_main, actual_pb)
                scored.append({
                    **s,
                    "main_numbers": r["main_numbers"],
                    "powerball": int(r["powerball"]),
                    "confidence_score_0_100": float(r.get("confidence_score_0_100", np.nan)),
                    "game_rank_for_draw": int(r.get("rank", r.get("game_rank_for_draw", 1))),
                })
            sdf = pd.DataFrame(scored).sort_values(
                ["total_match_score", "main_matches", "powerball_match", "game_rank_for_draw"],
                ascending=[False, False, False, True]
            ).reset_index(drop=True)

            best = sdf.iloc[0]
            return {
                "model": model_name,
                "best_main_matches": int(best["main_matches"]),
                "best_powerball_match": int(best["powerball_match"]),
                "best_total_match_score": float(best["total_match_score"]),
                "avg_main_matches": float(sdf["main_matches"].mean()),
                "avg_powerball_match": float(sdf["powerball_match"].mean()),
                "avg_total_match_score": float(sdf["total_match_score"].mean()),
                "top_card_main_numbers": best["main_numbers"],
                "top_card_powerball": int(best["powerball"]),
                "top_card_confidence": float(best["confidence_score_0_100"]) if not pd.isna(best["confidence_score_0_100"]) else np.nan,
                "rule_count_main": int(len(main_rules_used)) if model_name == "planet" else 0,
                "rule_count_powerball": int(len(pb_rules_used)) if model_name == "planet" else 0,
            }

        planet_eval = eval_cards(planet_cards, "planet")
        baseline_eval = eval_cards(baseline_cards, "baseline")

        # Save one row per model for this test draw
        for ev in [planet_eval, baseline_eval]:
            detail_rows.append({
                "test_draw_id": int(test_row["draw_id"]),
                "test_draw_datetime_local": test_row["draw_datetime_local"],
                "test_draw_datetime_utc": test_row["draw_datetime_utc"],
                "actual_main_numbers": actual_main,
                "actual_powerball": actual_pb,
                **ev,
            })

    detail_df = pd.DataFrame(detail_rows)

    if len(detail_df) == 0:
        return detail_df, pd.DataFrame([{"error": "No backtest rows generated"}])

    # Summary by model
    summary_df = (
        detail_df
        .groupby("model", dropna=False)
        .agg(
            test_draws=("test_draw_id", "nunique"),
            mean_best_main_matches=("best_main_matches", "mean"),
            mean_best_powerball_match=("best_powerball_match", "mean"),
            mean_best_total_match_score=("best_total_match_score", "mean"),
            mean_avg_main_matches=("avg_main_matches", "mean"),
            mean_avg_powerball_match=("avg_powerball_match", "mean"),
            mean_avg_total_match_score=("avg_total_match_score", "mean"),
            max_main_matches_seen=("best_main_matches", "max"),
            powerball_hits_total=("best_powerball_match", "sum"),
        )
        .reset_index()
    )

    # Side-by-side comparison convenience metrics
    if set(summary_df["model"]) >= {"planet", "baseline"}:
        p = summary_df[summary_df["model"] == "planet"].iloc[0]
        b = summary_df[summary_df["model"] == "baseline"].iloc[0]
        compare = pd.DataFrame([{
            "metric": "mean_best_main_matches",
            "planet": float(p["mean_best_main_matches"]),
            "baseline": float(b["mean_best_main_matches"]),
            "delta_planet_minus_baseline": float(p["mean_best_main_matches"] - b["mean_best_main_matches"]),
        }, {
            "metric": "mean_best_powerball_match",
            "planet": float(p["mean_best_powerball_match"]),
            "baseline": float(b["mean_best_powerball_match"]),
            "delta_planet_minus_baseline": float(p["mean_best_powerball_match"] - b["mean_best_powerball_match"]),
        }, {
            "metric": "mean_best_total_match_score",
            "planet": float(p["mean_best_total_match_score"]),
            "baseline": float(b["mean_best_total_match_score"]),
            "delta_planet_minus_baseline": float(p["mean_best_total_match_score"] - b["mean_best_total_match_score"]),
        }])
        summary_df = {"by_model": summary_df, "comparison": compare}
    else:
        summary_df = {"by_model": summary_df, "comparison": pd.DataFrame()}

    return detail_df, summary_df


def summarize_backtest_plain_english(detail_df: pd.DataFrame, summary_obj):
    if detail_df is None or len(detail_df) == 0:
        return "No backtest results were generated. Check that you have enough modern-format historical draws and feature rows."

    by_model = summary_obj.get("by_model", pd.DataFrame()) if isinstance(summary_obj, dict) else summary_obj
    comp = summary_obj.get("comparison", pd.DataFrame()) if isinstance(summary_obj, dict) else pd.DataFrame()

    if len(by_model) == 0:
        return "Backtest ran, but summary metrics were not available."

    lines = []
    lines.append(f"Walk-forward backtest completed on {detail_df['test_draw_id'].nunique()} historical draws using no-lookahead training (each draw predicted using only earlier draws).")

    for _, r in by_model.iterrows():
        lines.append(
            f"- {str(r['model']).title()} model: average best-card main matches = {float(r['mean_best_main_matches']):.2f}, "
            f"Powerball hit rate (best of generated cards) = {float(r['mean_best_powerball_match']):.2f} per draw, "
            f"average best total match score = {float(r['mean_best_total_match_score']):.2f}."
        )

    if len(comp) > 0:
        # Key comparison: total match score
        row = comp[comp["metric"] == "mean_best_total_match_score"]
        if len(row):
            d = float(row.iloc[0]["delta_planet_minus_baseline"])
            if d > 0.05:
                lines.append(f"The planet-feature model outperformed the baseline on average total match score by {d:.2f} points in this backtest window.")
            elif d < -0.05:
                lines.append(f"The planet-feature model underperformed the baseline by {abs(d):.2f} points on average total match score in this backtest window.")
            else:
                lines.append("The planet-feature model performed similarly to the baseline in average total match score (no clear edge in this test window).")

    lines.append("This backtest is an exploratory validation step. If results look promising, the next improvement is score calibration and repeated backtests across multiple test windows.")
    return "\n".join(lines)